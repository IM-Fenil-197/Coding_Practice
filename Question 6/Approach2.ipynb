# -*- coding: utf-8 -*-
"""PDF_load_Falcon_7B_instruct

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1_FcLegrKwUdL0jz5PglQj1L3NBduczVI
"""

! pip install --upgrade langchain

! pip install InstructorEmbedding

!pip install tiktoken

! pip install sentence_transformers

!pip install "langchain[docarray]"

! pip install chromadb

!pip install langchain==0.0.216

! pip install --force-reinstall typing-extensions==4.5.0

! python -m pip install -U pydantic spacy

! pip install pypdf

from langchain.llms import HuggingFaceHub

from langchain.embeddings import HuggingFaceInstructEmbeddings
from langchain.prompts import PromptTemplate
from langchain.text_splitter import CharacterTextSplitter

import os
import pandas as pd
import http.client
import json
import langchain

from langchain.vectorstores import DocArrayInMemorySearch
from IPython.display import display, Markdown
from langchain.document_loaders import PyPDFLoader
from langchain.chains import RetrievalQA
from langchain.chains import LLMChain

from langchain.vectorstores import Chroma
from langchain.indexes import VectorstoreIndexCreator

## Model used for embeddings. And this is giving me perfect results

model_name = "hkunlp/instructor-large"
model_kwargs = {'device': 'cuda'}
encode_kwargs = {'normalize_embeddings': True}
hf = HuggingFaceInstructEmbeddings(
    model_name=model_name,
    model_kwargs=model_kwargs,
    encode_kwargs=encode_kwargs
)

loader = PyPDFLoader("/content/Family_Care_by_W.W.Jacobs.pdf")
text_splitter = CharacterTextSplitter(chunk_size=500, chunk_overlap=20)
pages = loader.load_and_split(text_splitter=text_splitter)

pages[1].page_content

## Creating a database from the texts
db = Chroma.from_documents(pages, hf)

retriever = db.as_retriever()

## Initiating the Falcon-7b-instruct model
llm = HuggingFaceHub(repo_id="tiiuae/falcon-7b-instruct", huggingfacehub_api_token="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")

prompt_template = """ Understand data and context and text
{context}

Question: {question}
Answer:"""
PROMPT = PromptTemplate(
    template=prompt_template, input_variables=["context", "question"],
)

chain_type_kwargs = {"prompt": PROMPT}
qa = RetrievalQA.from_chain_type(llm=llm, chain_type="stuff", retriever=retriever, chain_type_kwargs=chain_type_kwargs, return_source_documents=True)

query1 = "Who is author of Book?"
# qa.run(query)

result1 = qa({"query": query2})

result1["result"]

query2 = "What are different characters are there described into books?"
result2 = qa({"query": query2})
result2["result"]

query3 = "What is the main concern of Mr. Barrett in this passage?"
result3 = qa({"query": query3})
result3["result"]

